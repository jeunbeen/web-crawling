{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 세팅\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.parse import quote, urlencode, quote_plus, unquote\n",
    "import json\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import datetime as dt\n",
    "import time as t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "\n",
    "#from tqdm.notebook import tqdm as tq\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 날짜 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''1. 목표 날짜 지정  (검색할 날짜)'''\n",
    "# 검색 시작 날짜    \n",
    "first_date = 20200201\n",
    "current_date = (int(str(first_date)[:4]) , int(str(first_date)[4:6]), int(str(first_date)[6:]))\n",
    "\n",
    "# 검색 종료 날짜\n",
    "end_date = 20200205\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200201\n",
      "20200202\n",
      "20200203\n",
      "20200204\n",
      "20200205\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4a2507111d4d7b84358bee8c1e2f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "##### 1. url 크롤링                #####\n",
    "########################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############\n",
    "data_set = {}          # 데이터가 담길 dictionary\n",
    "############## \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# first_date 에서 날짜 넘겨받는 부분  (시작점)\n",
    "current_date = (int(str(first_date)[:4]) , int(str(first_date)[4:6]), int(str(first_date)[6:]))\n",
    "current_date_WHILE = int(dt.datetime(current_date[0], current_date[1], current_date[2]).strftime('%Y%m%d'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''1. 날짜 하루씩 추가하면서 크롤링 진행'''\n",
    "# '날짜 지정'에서 설정한 검색종료 날짜까지 작업 반복\n",
    "while current_date_WHILE <=end_date :\n",
    "    print(current_date_WHILE)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    2. current_date 를 기반으로 날짜 변수를 datetime 형태로 저장  -\n",
    "    ->    이후의 날짜 계산을 용이하게 하기 위함\n",
    "    '''\n",
    "    s_date = dt.datetime(current_date[0], current_date[1], current_date[2]) \n",
    "    e_date = s_date + dt.timedelta(days=1)   \n",
    "\n",
    "\n",
    "    '''\n",
    "    3. datetime 날짜의 type 조작   -->    검색창에 입력할 수 있는 형태로 변환\n",
    "    '''\n",
    "\n",
    "    # 1.  20200201  형태로 저장\n",
    "    str_s_date1 = s_date.strftime('%Y%m%d')\n",
    "    str_e_date1 = e_date.strftime('%Y%m%d')\n",
    "\n",
    "    # 2,  2020.02.01 형태로 저장\n",
    "    str_s_date2 = s_date.strftime('%Y.%m.%d')\n",
    "    str_e_date2 = e_date.strftime('%Y.%m.%d')\n",
    "\n",
    "\n",
    "    '''\n",
    "    4. url 및 검색 parameter 지정\n",
    "    '''\n",
    "\n",
    "    # base url\n",
    "    base_url = 'https://search.naver.com/search.naver'\n",
    "\n",
    "    # parameters\n",
    "    query_params = '?' + urlencode(\n",
    "        {\n",
    "            quote_plus('where') : 'news',\n",
    "            quote_plus('query') : \"+코로나 창업 | 스타트업\",\n",
    "            quote_plus('sm') : 'tab_opt',\n",
    "            quote_plus('') : '',\n",
    "            quote_plus('sort')  : '2',\n",
    "            quote_plus('photo') : '0',\n",
    "            quote_plus('field') : '0',\n",
    "            quote_plus('pd') : '3',\n",
    "            quote_plus('ds') : str_s_date2,\n",
    "            quote_plus('de') : str_e_date2,\n",
    "            quote_plus('docid') : '',  \n",
    "            quote_plus('related') : '0',\n",
    "            quote_plus('mynews') : '1',       # 언론사 선택시 1 / default = 0 \n",
    "            quote_plus('office_type') :  '1',  # default = 0 / 일간지 = 1 \n",
    "            quote_plus('office_section_code') : '1',  # 언론사 선택하면 1, default = 0\n",
    "            \n",
    "            ########################\n",
    "            ### 신문사 코드 입력 ###\n",
    "            ########################\n",
    "            \n",
    "            quote_plus('news_office_checked') : '1023',  #중앙일보 1023,  만일 없으면 공백으로 비워두어야 함\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 첫번째 반복시행에서 article index는 1 고정\n",
    "    first_article_index = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###############################  특정 날짜에서의 검색창 크롤링 ###########################\n",
    "    #iteration_idx = 0\n",
    "    while True:    \n",
    "\n",
    "        \n",
    "        try:\n",
    "            #debug code\n",
    "            #iteration_idx +=1\n",
    "            #print('while 작업 횟수 : ', iteration_idx)\n",
    "\n",
    "            '''\n",
    "            5. 뉴스 검색창 접속\n",
    "            '''\n",
    "            # 기타 parameter\n",
    "            start = '&start=' + str(first_article_index)  # 페이지 첫번째 기사의 index, \n",
    "                                    # 처음은 1로 시작하나 페이지 넘길 때마다 10씩 증가시켜야 함\n",
    "                                    # 즉, 날짜별 iteration 코드에 < start += 10 >  를 넣어주어야 함\n",
    "            query_others = '&nso=so%3Ar%2Cp%3Afrom' + str_s_date1 + 'to' + str_e_date1 + '&is_sug_officeid=0' + start\n",
    "\n",
    "            # url\n",
    "            url = base_url + query_params + query_others\n",
    "            # 서버에 접속. 너무 급한 속도의 접속은 IP ban을 유발할 수 있음\n",
    "            req = Request(url, headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "            )\n",
    "            page = urlopen(req)\n",
    "            \n",
    "        except:\n",
    "            t.sleep(20)\n",
    "\n",
    "\n",
    "\n",
    "            '''\n",
    "            5. 뉴스 검색창 접속\n",
    "            '''\n",
    "            # 기타 parameter\n",
    "            start = '&start=' + str(first_article_index)  # 페이지 첫번째 기사의 index, \n",
    "                                    # 처음은 1로 시작하나 페이지 넘길 때마다 10씩 증가시켜야 함\n",
    "                                    # 즉, 날짜별 iteration 코드에 < start += 10 >  를 넣어주어야 함\n",
    "            query_others = '&nso=so%3Ar%2Cp%3Afrom' + str_s_date1 + 'to' + str_e_date1 + '&is_sug_officeid=0' + start\n",
    "\n",
    "            # url\n",
    "            url = base_url + query_params + query_others\n",
    "            # 서버에 접속. 너무 급한 속도의 접속은 IP ban을 유발할 수 있음\n",
    "            req = Request(url, headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "            )\n",
    "            page = urlopen(req)\n",
    "\n",
    "            \n",
    "            \n",
    "        t.sleep(0.3)     # IP ban 방지용\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        6. page parsing\n",
    "        '''\n",
    "        \n",
    "        # bs parsing\n",
    "        page = bs(page, 'html.parser')\n",
    "\n",
    "        # IS_EMPTY 는 검색결과가 있다면 [] 를 반환함. \n",
    "        # 만일 IS_EMPTY 가 []가 아니라면 검색결과가 없는 것이므로 break 해야 함.\n",
    "        IS_EMPTY = page.find_all('div', 'api_noresult_wrap')\n",
    "        if IS_EMPTY != []:\n",
    "            break \n",
    "\n",
    "        # 필요 데이터 parsing (url, 날짜)\n",
    "        url_resultset = page.find_all('div', 'news_info')   #  --> url\n",
    "        news_date_resultset = page.find_all('div', 'info_group')  # --> 날짜,  둘다 parsing 후의 index는 같음\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        7. 지면뉴스 오류 handling\n",
    "        '''\n",
    "        for idx in range(len(url_resultset)):\n",
    "\n",
    "            # url\n",
    "            target_url = url_resultset[idx].a['data-url']\n",
    "\n",
    "            # 날짜\n",
    "            if news_date_resultset[idx].i:\n",
    "                IS_PAPERNEWS = True\n",
    "            else:\n",
    "                IS_PAPERNEWS = False\n",
    "\n",
    "            if IS_PAPERNEWS ==False:\n",
    "                target_date = news_date_resultset[idx].find('span', 'info').text\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    target_date = news_date_resultset[idx].find_all('span', 'info')[1].text\n",
    "                except:\n",
    "                    target_date = news_date_resultset[idx].find_all('span', 'info')[0].text\n",
    "\n",
    "\n",
    "            # 데이터 저장, key = url  / value = date\n",
    "            data_set[target_url] = target_date\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # 다음 검색 페이지로 넘어가기 위해 기사 index 조정\n",
    "        first_article_index += 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 반복문 break에 사용할 current_date 설정\n",
    "    current_date = s_date\n",
    "    current_date += dt.timedelta(days = 1)  # 하루 더하기\n",
    "    current_date = current_date.strftime('%Y%m%d')  # 더한 날짜정보를 str으로 바꾸기\n",
    "\n",
    "    # str으로 변환된 날짜정보를 연,월,일로 분리\n",
    "    year = current_date[:4]\n",
    "    month = current_date[4:6]\n",
    "    day = current_date[6:]\n",
    "\n",
    "    # 다음 iteration에 넘어갈 current_date 정보\n",
    "    current_date = (int(year), int(month), int(day))\n",
    "\n",
    "\n",
    "    # 날짜 while문 종료에 관한 정보\n",
    "    current_date_WHILE = int(dt.datetime(current_date[0], current_date[1], current_date[2]).strftime('%Y%m%d'))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "'''\n",
    "데이터 내보내기 (url)\n",
    "'''     \n",
    "        \n",
    "table = pd.DataFrame(data_set.keys(), data_set.values())\n",
    "table = table.drop_duplicates()\n",
    "table.to_csv('중앙일보 url주소.csv', encoding = 'cp949')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "##### 2. 본문 크롤링               #####\n",
    "########################################\n",
    "\n",
    "\n",
    "\n",
    "# url 가져오기 (csv파일)\n",
    "\n",
    "urls = list(pd.read_csv('중앙일보 url주소.csv', encoding = 'cp949')['0'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 데이터가 들어갈 공간\n",
    "data_set = {}\n",
    "\n",
    "\n",
    "'''\n",
    "data columns :\n",
    "    date   / title  / body   / company  / url\n",
    "    (날짜) / (제목) / (본문) / (언론사) / (url)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#url 접속 후 크롤링\n",
    "\n",
    "\n",
    "for idx in range(len(urls)):\n",
    "    \n",
    "    # 차단 막기 위해 잠시 휴식\n",
    "    #t.sleep(0.3)\n",
    "    \n",
    "    # 초기화\n",
    "    body = ''\n",
    "    title = ''\n",
    "    day = ''\n",
    "    company = ''\n",
    "\n",
    "    \n",
    "    # 중앙일보\n",
    "    if 'chosun.com' in urls[idx]:\n",
    "        \n",
    "        \n",
    "        # '여성조선'은 구현하지 않음\n",
    "        if 'woman.chosun.com' in urls[idx]:\n",
    "            continue\n",
    "        # 'IT 조선'은 구현하지 않음\n",
    "        if 'it.chosun.com' in urls[idx]:\n",
    "            continue\n",
    "        # 'tv조선'은 구현하지 않음\n",
    "        if 'tvchosun.com' in urls[idx]:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### 접속 ###\n",
    "        try: \n",
    "            req = Request(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "            )\n",
    "            t.sleep(0.3)\n",
    "            page = urlopen(req)\n",
    "            page = bs(page, 'html.parser')\n",
    "\n",
    "        except:\n",
    "            # RemoteDiscunnected exception handling\n",
    "            t.sleep(20)\n",
    "            req = Request(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "            )\n",
    "            t.sleep(0.3)\n",
    "            page = urlopen(req)\n",
    "            page = bs(page, 'html.parser')\n",
    "            \n",
    "            \n",
    "            \n",
    "        ### parsing ###\n",
    "        resultset = page.find_all()[0].decode_contents().split('_id')\n",
    "        temp = []\n",
    "        for line in resultset:\n",
    "            if ('content' in line) and ('\"type\":\"text\"' in line) :\n",
    "                # 기사 head에 해당하는 부분이므로 제거\n",
    "                if 'strong' in line:\n",
    "                    continue\n",
    "                # 본문 부분만 긁어서 한 문장으로 합치기\n",
    "                if line.rsplit(\"content\")[1]:\n",
    "                    target = line.split('\"content\":\"')[1].split('\",\"type')[0]    \n",
    "                    temp.append(target)\n",
    "\n",
    "        ### 데이터 저장 ###\n",
    "        body = \"\".join(temp).replace('\\\\', \"\")        \n",
    "        title = page.text.replace('\\n', \"\")\n",
    "        day = page.body.script.decode_contents().split('\"display_date\":\"')[1].split(',\"distributor\"')[0].split('T')[0]\n",
    "        company = '중앙일보'\n",
    "        data_set[idx] = [day, title, body, company, urls[idx]]\n",
    "\n",
    "    # 다른신문사는 제외 (exception 예방)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "'''\n",
    "데이터 내보내기 (뉴스본문)\n",
    "'''\n",
    "        \n",
    "dataframe = pd.DataFrame(data_set).transpose()\n",
    "dataframe.columns = ['date', 'title', 'body', 'company', 'url']\n",
    "# 본문 내 특수문자 제거\n",
    "dataframe['body'] = dataframe['body'].str.replace(pat=r'[^\\w\\s\\.]', repl=r'', regex=True)\n",
    "dataframe['body'] = dataframe['body'].str.replace(pat=r'[\"\\xa0\"]', repl = r'', regex=True)\n",
    "# 한자 제거\n",
    "dataframe['body'] = dataframe['body'].str.replace(pat=r'[\\u3400-\\u9FBF]', repl = r'', regex=True)\n",
    "dataframe['body'] = dataframe['body'].str.replace(' b ', '')\n",
    "\n",
    "dataframe = dataframe.reset_index()\n",
    "dataframe.to_csv('중앙일보_{}to{}.csv'.format(first_date, end_date), encoding = \"cp949\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
