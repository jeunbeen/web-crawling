{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\r\n",
    "# install pakages  (  -> first time )\r\n",
    "\r\n",
    "\r\n",
    "!pip install urllib3\r\n",
    "!pip install bs4\r\n",
    "!pip install pandas\r\n",
    "!pip install tqdm\r\n",
    "\r\n",
    "\r\n",
    "# 기본 세팅\r\n",
    "\r\n",
    "from urllib.error import HTTPError\r\n",
    "import requests\r\n",
    "import re\r\n",
    "\r\n",
    "from urllib.request import Request, urlopen\r\n",
    "from urllib.parse import quote, urlencode, quote_plus, unquote\r\n",
    "from bs4 import BeautifulSoup as bs\r\n",
    "\r\n",
    "\r\n",
    "import copy\r\n",
    "import os\r\n",
    "import datetime as dt\r\n",
    "import time as t\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import random as rd\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings(\"ignore\")\r\n",
    "\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "import ssl\r\n",
    "from ssl import SSLError\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: urllib3 in c:\\users\\admin\\anaconda3\\envs\\nlp\\lib\\site-packages (1.26.6)\n",
      "Requirement already satisfied: bs4 in c:\\users\\admin\\anaconda3\\envs\\nlp\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\admin\\anaconda3\\envs\\nlp\\lib\\site-packages (from bs4) (4.6.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\anaconda3\\envs\\nlp\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\admin\\anaconda3\\envs\\nlp\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\admin\\anaconda3\\envs\\nlp\\lib\\site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\admin\\anaconda3\\envs\\nlp\\lib\\site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\envs\\nlp\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\envs\\nlp\\lib\\site-packages (4.62.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\nlp\\lib\\site-packages (from tqdm) (0.4.4)\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 동아일보 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "start_time  = t.time()\r\n",
    "\r\n",
    "for is_data in range(1):\r\n",
    "    \r\n",
    "    # url 가져오기 (csv파일)\r\n",
    "    url_data = pd.read_csv('전체기사 url주소.csv', encoding = 'cp949')\r\n",
    "    \r\n",
    "    if len(url_data)==0:\r\n",
    "        print('기사가 없습니다.')\r\n",
    "        continue\r\n",
    "    else:\r\n",
    "        url_data.columns = ['date', 'urls']\r\n",
    "\r\n",
    "        # 특정 url만 필터링\r\n",
    "        url_data = url_data[ url_data['urls'].str.contains('donga')]\r\n",
    "\r\n",
    "        url_data = url_data.reset_index()\r\n",
    "        url_data.columns = ['index', 'date', 'urls']\r\n",
    "        urls = list(url_data['urls'])\r\n",
    "\r\n",
    "    # 데이터가 들어갈 공간\r\n",
    "    data_set = {}\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    #url 접속 후 크롤링\r\n",
    "    print(\"### 본문 수집 ###\")\r\n",
    "    for idx in tqdm(range(len(urls))):\r\n",
    "\r\n",
    "        # 초기화\r\n",
    "        body = ''\r\n",
    "        title = ''\r\n",
    "        day = ''\r\n",
    "        company = ''\r\n",
    "\r\n",
    "\r\n",
    "        # 동아일보 외 타 신문사 제외\r\n",
    "        if 'www.donga.com' in urls[idx]:\r\n",
    "\r\n",
    "\r\n",
    "            ### parsing ###\r\n",
    "            try:\r\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\r\n",
    "                )\r\n",
    "                t.sleep(0.3)\r\n",
    "                page = req.content\r\n",
    "                page = bs(page, 'html.parser')  \r\n",
    "\r\n",
    "            except HTTPError:\r\n",
    "\r\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\r\n",
    "                continue\r\n",
    "\r\n",
    "            except:\r\n",
    "\r\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\r\n",
    "                t.sleep(20)\r\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\r\n",
    "                )\r\n",
    "                t.sleep(0.3)\r\n",
    "                page = req.content\r\n",
    "                page = bs(page, 'html.parser')   \r\n",
    "                \r\n",
    "            body = page.find_all('div', 'article_view')[0].find_all('div', 'article_txt')[0].text.split('donga.com')[0].rsplit('기자')[0].lstrip()\r\n",
    "            title = page.title.text\r\n",
    "            day = url_data['date'][idx]\r\n",
    "            data_set[idx] = [day, title, body, company, urls[idx]]\r\n",
    "                \r\n",
    "            \r\n",
    "        else:\r\n",
    "            continue\r\n",
    "            \r\n",
    "\r\n",
    "    '''\r\n",
    "    데이터 내보내기 (뉴스본문)\r\n",
    "    '''\r\n",
    "    dataframe = pd.DataFrame(data_set).transpose()\r\n",
    "    if len(dataframe) ==0:\r\n",
    "        print('자료가 없습니다.')\r\n",
    "        pass\r\n",
    "    else:\r\n",
    "        dataframe.columns = ['date', 'title', 'body', 'company', 'url']\r\n",
    "        # 본문 내 특수문자 제거\r\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[^\\w\\s\\.\\%\\)\\(\\']', repl=r'', regex=True)\r\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\"\\xa0\"]', repl = r'', regex=True)\r\n",
    "        # 한자 제거\r\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\\u3400-\\u9FBF]', repl = r'', regex=True)\r\n",
    "        dataframe['body']= dataframe['body'].str.replace(pat=r'[\"\\u2027\",\"\\u2026\"]', repl = r'', regex=True)\r\n",
    "        dataframe['body'] = dataframe['body'].str.replace(' b ', '')\r\n",
    "        dataframe = dataframe.reset_index()\r\n",
    "    dataframe.to_csv('동아일보 크롤링.csv', encoding = \"cp949\", index= False)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "################################################\r\n",
    "end_time = t.time()\r\n",
    "duration = round((end_time - start_time)/60, 2)\r\n",
    "\r\n",
    "\r\n",
    "print('작업 완료!')\r\n",
    "print('작업 소요시간 : 약 {}분'.format(duration))\r\n",
    "print('기사 개수 : ', len(dataframe))\r\n",
    "print('작업 완료 시간 : ', str(dt.datetime.now()).split(' ')[0], str(dt.datetime.now()).split(' ')[1].split('.')[0])\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "### 본문 수집 ###\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 18/18 [00:01<00:00, 10.18it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "작업 완료!\n",
      "작업 소요시간 : 약 0.03분\n",
      "기사 개수 :  3\n",
      "작업 완료 시간 :  2021-09-14 11:02:16\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 중앙일보"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "start_time  = t.time()\r\n",
    "\r\n",
    "for is_data in range(1):\r\n",
    "    \r\n",
    "    # url 가져오기 (csv파일)\r\n",
    "    url_data = pd.read_csv('전체기사 url주소.csv', encoding = 'cp949')\r\n",
    "    \r\n",
    "    if len(url_data)==0:\r\n",
    "        print('기사가 없습니다.')\r\n",
    "        continue\r\n",
    "    else:\r\n",
    "        url_data.columns = ['date', 'urls']\r\n",
    "\r\n",
    "        # 특정 url만 필터링\r\n",
    "        url_data = url_data[ url_data['urls'].str.contains('joongang.co.kr')]\r\n",
    "\r\n",
    "        url_data = url_data.reset_index()\r\n",
    "        url_data.columns = ['index', 'date', 'urls']\r\n",
    "        urls = list(url_data['urls'])\r\n",
    "\r\n",
    "    # 데이터가 들어갈 공간\r\n",
    "    data_set = {}\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    #url 접속 후 크롤링\r\n",
    "    print(\"### 본문 수집 ###\")\r\n",
    "    for idx in tqdm(range(len(urls))):\r\n",
    "\r\n",
    "        # 초기화\r\n",
    "        body = ''\r\n",
    "        title = ''\r\n",
    "        day = ''\r\n",
    "        company = ''\r\n",
    "\r\n",
    "\r\n",
    "        # 중앙일보 외 타 신문사 제외\r\n",
    "        if 'www.joongang.co.kr' in urls[idx]:\r\n",
    "\r\n",
    "\r\n",
    "            ### parsing ###\r\n",
    "            try:\r\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\r\n",
    "                )\r\n",
    "                t.sleep(0.3)\r\n",
    "                page = req.content\r\n",
    "                page = bs(page, 'html.parser')  \r\n",
    "\r\n",
    "            except HTTPError:\r\n",
    "\r\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\r\n",
    "                continue\r\n",
    "\r\n",
    "            except:\r\n",
    "\r\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\r\n",
    "                t.sleep(20)\r\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\r\n",
    "                )\r\n",
    "                t.sleep(0.3)\r\n",
    "                page = req.content\r\n",
    "                page = bs(page, 'html.parser')   \r\n",
    "                \r\n",
    "            body = page.find_all('div', 'article_body')[0].text\r\n",
    "            title = page.title.text\r\n",
    "            day = url_data['date'][idx]\r\n",
    "            data_set[idx] = [day, title, body, company, urls[idx]]\r\n",
    "                \r\n",
    "            \r\n",
    "        else:\r\n",
    "            continue\r\n",
    "            \r\n",
    "\r\n",
    "    '''\r\n",
    "    데이터 내보내기 (뉴스본문)\r\n",
    "    '''\r\n",
    "    dataframe = pd.DataFrame(data_set).transpose()\r\n",
    "    if len(dataframe) ==0:\r\n",
    "        print('자료가 없습니다.')\r\n",
    "        pass\r\n",
    "    else:\r\n",
    "        dataframe.columns = ['date', 'title', 'body', 'company', 'url']\r\n",
    "        # 본문 내 특수문자 제거\r\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[^\\w\\s\\.\\%\\)\\(\\']', repl=r'', regex=True)\r\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\"\\xa0\"]', repl = r'', regex=True)\r\n",
    "        # 한자 제거\r\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\\u3400-\\u9FBF]', repl = r'', regex=True)\r\n",
    "        dataframe['body']= dataframe['body'].str.replace(pat=r'[\"\\u2027\",\"\\u2026\"]', repl = r'', regex=True)\r\n",
    "        dataframe['body'] = dataframe['body'].str.replace(' b ', '')\r\n",
    "        dataframe = dataframe.reset_index()\r\n",
    "    dataframe.to_csv('중앙일보 크롤링.csv', encoding = \"cp949\", index= False)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "################################################\r\n",
    "end_time = t.time()\r\n",
    "duration = round((end_time - start_time)/60, 2)\r\n",
    "\r\n",
    "\r\n",
    "print('작업 완료!')\r\n",
    "print('작업 소요시간 : 약 {}분'.format(duration))\r\n",
    "print('기사 개수 : ', len(dataframe))\r\n",
    "print('작업 완료 시간 : ', str(dt.datetime.now()).split(' ')[0], str(dt.datetime.now()).split(' ')[1].split('.')[0])\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "### 본문 수집 ###\n",
      "자료가 없습니다.\n",
      "작업 완료!\n",
      "작업 소요시간 : 약 0.0분\n",
      "기사 개수 :  0\n",
      "작업 완료 시간 :  2021-09-13 12:04:54\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 한겨레"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "start_time  = t.time()\r\n",
    "\r\n",
    "for is_data in range(1):\r\n",
    "    \r\n",
    "    # url 가져오기 (csv파일)\r\n",
    "    url_data = pd.read_csv('전체기사 url주소.csv', encoding = 'cp949')\r\n",
    "    \r\n",
    "    if len(url_data)==0:\r\n",
    "        print('기사가 없습니다.')\r\n",
    "        continue\r\n",
    "    else:\r\n",
    "        url_data.columns = ['date', 'urls']\r\n",
    "\r\n",
    "        # 특정 url만 필터링\r\n",
    "        url_data = url_data[ url_data['urls'].str.contains('hani.co.kr')]\r\n",
    "\r\n",
    "        url_data = url_data.reset_index()\r\n",
    "        url_data.columns = ['index', 'date', 'urls']\r\n",
    "        urls = list(url_data['urls'])\r\n",
    "\r\n",
    "    # 데이터가 들어갈 공간\r\n",
    "    data_set = {}\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    #url 접속 후 크롤링\r\n",
    "    print(\"### 본문 수집 ###\")\r\n",
    "    for idx in tqdm(range(len(urls))):\r\n",
    "\r\n",
    "        # 초기화\r\n",
    "        body = ''\r\n",
    "        title = ''\r\n",
    "        day = ''\r\n",
    "        company = ''\r\n",
    "\r\n",
    "\r\n",
    "        # 한겨레 외 타 신문사 제외\r\n",
    "        if 'www.hani.co.kr' in urls[idx]:\r\n",
    "\r\n",
    "\r\n",
    "            ### parsing ###\r\n",
    "            try:\r\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\r\n",
    "                )\r\n",
    "                t.sleep(0.3)\r\n",
    "                page = req.content\r\n",
    "                page = bs(page, 'html.parser')  \r\n",
    "\r\n",
    "            except HTTPError:\r\n",
    "\r\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\r\n",
    "                continue\r\n",
    "\r\n",
    "            except:\r\n",
    "\r\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\r\n",
    "                t.sleep(20)\r\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\r\n",
    "                )\r\n",
    "                t.sleep(0.3)\r\n",
    "                page = req.content\r\n",
    "                page = bs(page, 'html.parser')   \r\n",
    "                \r\n",
    "            body = page.find_all('div', 'article-text')[0].text.rsplit('@hani.co.kr', 1)[0]\r\n",
    "            title = page.title.text\r\n",
    "            day = url_data['date'][idx]\r\n",
    "            data_set[idx] = [day, title, body, company, urls[idx]]\r\n",
    "                \r\n",
    "            \r\n",
    "        else:\r\n",
    "            continue\r\n",
    "            \r\n",
    "\r\n",
    "    '''\r\n",
    "    데이터 내보내기 (뉴스본문)\r\n",
    "    '''\r\n",
    "    dataframe = pd.DataFrame(data_set).transpose()\r\n",
    "    if len(dataframe) ==0:\r\n",
    "        print('자료가 없습니다.')\r\n",
    "        pass\r\n",
    "    else:\r\n",
    "        dataframe.columns = ['date', 'title', 'body', 'company', 'url']\r\n",
    "        # 본문 내 특수문자 제거\r\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[^\\w\\s\\.\\%\\)\\(\\']', repl=r'', regex=True)\r\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\"\\xa0\"]', repl = r'', regex=True)\r\n",
    "        # 한자 제거\r\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\\u3400-\\u9FBF]', repl = r'', regex=True)\r\n",
    "        dataframe['body']= dataframe['body'].str.replace(pat=r'[\"\\u2027\",\"\\u2026\"]', repl = r'', regex=True)\r\n",
    "        dataframe['body'] = dataframe['body'].str.replace(' b ', '')\r\n",
    "        dataframe = dataframe.reset_index()\r\n",
    "    dataframe.to_csv('한겨레 크롤링.csv', encoding = \"cp949\", index= False)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "################################################\r\n",
    "end_time = t.time()\r\n",
    "duration = round((end_time - start_time)/60, 2)\r\n",
    "\r\n",
    "\r\n",
    "print('작업 완료!')\r\n",
    "print('작업 소요시간 : 약 {}분'.format(duration))\r\n",
    "print('기사 개수 : ', len(dataframe))\r\n",
    "print('작업 완료 시간 : ', str(dt.datetime.now()).split(' ')[0], str(dt.datetime.now()).split(' ')[1].split('.')[0])\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 't' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23316/2016667071.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstart_time\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mis_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# url 가져오기 (csv파일)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 't' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 경향신문"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "start_time  = t.time()\r\n",
    "\r\n",
    "for is_data in range(1):\r\n",
    "    \r\n",
    "    # url 가져오기 (csv파일)\r\n",
    "    url_data = pd.read_csv('전체기사 url주소.csv', encoding = 'cp949')\r\n",
    "    \r\n",
    "    if len(url_data)==0:\r\n",
    "        print('기사가 없습니다.')\r\n",
    "        continue\r\n",
    "    else:\r\n",
    "        url_data.columns = ['date', 'urls']\r\n",
    "\r\n",
    "        # 특정 url만 필터링\r\n",
    "        url_data = url_data[ url_data['urls'].str.contains('khan.co.kr')]\r\n",
    "\r\n",
    "        url_data = url_data.reset_index()\r\n",
    "        url_data.columns = ['index', 'date', 'urls']\r\n",
    "        urls = list(url_data['urls'])\r\n",
    "\r\n",
    "    # 데이터가 들어갈 공간\r\n",
    "    data_set = {}\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    #url 접속 후 크롤링\r\n",
    "    print(\"### 본문 수집 ###\")\r\n",
    "    for idx in tqdm(range(len(urls))):\r\n",
    "\r\n",
    "        # 초기화\r\n",
    "        body = ''\r\n",
    "        title = ''\r\n",
    "        day = ''\r\n",
    "        company = ''\r\n",
    "\r\n",
    "\r\n",
    "        # 경향 외 타 신문사 제외\r\n",
    "        if 'khan.co.kr' in urls[idx]:\r\n",
    "\r\n",
    "\r\n",
    "            ### parsing ###\r\n",
    "            try:\r\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\r\n",
    "                )\r\n",
    "                t.sleep(0.3)\r\n",
    "                page = req.content\r\n",
    "                page = bs(page, 'html.parser')  \r\n",
    "\r\n",
    "            except HTTPError:\r\n",
    "\r\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\r\n",
    "                continue\r\n",
    "\r\n",
    "            except:\r\n",
    "\r\n",
    "                urlopen(\"https://no-valid-cert\", context=context)\r\n",
    "                t.sleep(20)\r\n",
    "                req = requests.get(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\r\n",
    "                )\r\n",
    "                t.sleep(0.3)\r\n",
    "                page = req.content\r\n",
    "                page = bs(page, 'html.parser')   \r\n",
    "                \r\n",
    "\r\n",
    "            # body = page.find_all('div', 'art_body')[0].text\r\n",
    "            body = \"\"\r\n",
    "            for line in page.find_all('p', class_='content_text'):\r\n",
    "                body += line.get_text()\r\n",
    "            title = page.find_all('title')[0].text\r\n",
    "            day = url_data['date'][idx]\r\n",
    "            data_set[idx] = [day, title, body, company, urls[idx]]\r\n",
    "\r\n",
    "            \r\n",
    "        else:\r\n",
    "            continue\r\n",
    "            \r\n",
    "\r\n",
    "    '''\r\n",
    "    데이터 내보내기 (뉴스본문)\r\n",
    "    '''\r\n",
    "    dataframe = pd.DataFrame(data_set).transpose()\r\n",
    "    if len(dataframe) ==0:\r\n",
    "        print('자료가 없습니다.')\r\n",
    "        pass\r\n",
    "    else:\r\n",
    "        dataframe.columns = ['date', 'title', 'body', 'company', 'url']\r\n",
    "        # 본문 내 특수문자 제거\r\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[^\\w\\s\\.\\%\\)\\(\\']', repl=r'', regex=True)\r\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\"\\xa0\"]', repl = r'', regex=True)\r\n",
    "        # 한자 제거\r\n",
    "        dataframe['body'] = dataframe['body'].str.replace(pat=r'[\\u3400-\\u9FBF]', repl = r'', regex=True)\r\n",
    "        dataframe['body']= dataframe['body'].str.replace(pat=r'[\"\\u2027\",\"\\u2026\"]', repl = r'', regex=True)\r\n",
    "        dataframe['body'] = dataframe['body'].str.replace(' b ', '')\r\n",
    "        dataframe = dataframe.reset_index()\r\n",
    "    dataframe.to_csv('경향 크롤링.csv', encoding = \"cp949\", index= False)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "################################################\r\n",
    "end_time = t.time()\r\n",
    "duration = round((end_time - start_time)/60, 2)\r\n",
    "\r\n",
    "\r\n",
    "print('작업 완료!')\r\n",
    "print('작업 소요시간 : 약 {}분'.format(duration))\r\n",
    "print('기사 개수 : ', len(dataframe))\r\n",
    "print('작업 완료 시간 : ', str(dt.datetime.now()).split(' ')[0], str(dt.datetime.now()).split(' ')[1].split('.')[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "### 본문 수집 ###\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.70it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "작업 완료!\n",
      "작업 소요시간 : 약 0.04분\n",
      "기사 개수 :  4\n",
      "작업 완료 시간 :  2021-09-14 11:58:04\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('nlp': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "interpreter": {
   "hash": "3384638f5810346ec4e47c085d16c91a672dec4d6598dd6288d516a5e6f3bebf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}