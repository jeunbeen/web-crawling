{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 세팅\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.parse import quote, urlencode, quote_plus, unquote\n",
    "import json\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import datetime as dt\n",
    "import time as t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "\n",
    "#from tqdm.notebook import tqdm as tq\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 날짜 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''1. 목표 날짜 지정  (검색할 날짜)'''\n",
    "# 검색 시작 날짜    \n",
    "first_date = 20200201\n",
    "current_date = (int(str(first_date)[:4]) , int(str(first_date)[4:6]), int(str(first_date)[6:]))\n",
    "\n",
    "# 검색 종료 날짜\n",
    "end_date = 20200210\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작업 완료!\n",
      "작업 소요시간 : 약 0.18분\n",
      "작업 완료 시간 :  2021-09-08 17:34:57\n"
     ]
    }
   ],
   "source": [
    "start_time = t.time()\n",
    "\n",
    "########################################\n",
    "##### 1. url 크롤링                #####   -> '동아일보 url주소.csv' 생성\n",
    "########################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############\n",
    "data_set = {}          # 데이터가 담길 dictionary\n",
    "############## \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# first_date 에서 날짜 넘겨받는 부분  (시작점)\n",
    "current_date = (int(str(first_date)[:4]) , int(str(first_date)[4:6]), int(str(first_date)[6:]))\n",
    "current_date_WHILE = int(dt.datetime(current_date[0], current_date[1], current_date[2]).strftime('%Y%m%d'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''1. 날짜 하루씩 추가하면서 크롤링 진행'''\n",
    "# '날짜 지정'에서 설정한 검색종료 날짜까지 작업 반복\n",
    "while current_date_WHILE <=end_date :\n",
    "    # print(current_date_WHILE)\n",
    "    # debug\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    2. current_date 를 기반으로 날짜 변수를 datetime 형태로 저장  -\n",
    "    ->    이후의 날짜 계산을 용이하게 하기 위함\n",
    "    '''\n",
    "    s_date = dt.datetime(current_date[0], current_date[1], current_date[2]) \n",
    "    e_date = s_date + dt.timedelta(days=1)   \n",
    "\n",
    "\n",
    "    '''\n",
    "    3. datetime 날짜의 type 조작   -->    검색창에 입력할 수 있는 형태로 변환\n",
    "    '''\n",
    "\n",
    "    # 1.  20200201  형태로 저장\n",
    "    str_s_date1 = s_date.strftime('%Y%m%d')\n",
    "    str_e_date1 = e_date.strftime('%Y%m%d')\n",
    "\n",
    "    # 2,  2020.02.01 형태로 저장\n",
    "    str_s_date2 = s_date.strftime('%Y.%m.%d')\n",
    "    str_e_date2 = e_date.strftime('%Y.%m.%d')\n",
    "\n",
    "\n",
    "    '''\n",
    "    4. url 및 검색 parameter 지정\n",
    "    '''\n",
    "\n",
    "    # base url\n",
    "    base_url = 'https://search.naver.com/search.naver'\n",
    "\n",
    "    # parameters\n",
    "    query_params = '?' + urlencode(\n",
    "        {\n",
    "            quote_plus('where') : 'news',\n",
    "            quote_plus('query') : \"+코로나 창업 | 스타트업\",\n",
    "            quote_plus('sm') : 'tab_opt',\n",
    "            quote_plus('') : '',\n",
    "            quote_plus('sort')  : '2',\n",
    "            quote_plus('photo') : '0',\n",
    "            quote_plus('field') : '0',\n",
    "            quote_plus('pd') : '3',\n",
    "            quote_plus('ds') : str_s_date2,\n",
    "            quote_plus('de') : str_e_date2,\n",
    "            quote_plus('docid') : '',  \n",
    "            quote_plus('related') : '0',\n",
    "            quote_plus('mynews') : '1',       # 언론사 선택시 1 / default = 0 \n",
    "            quote_plus('office_type') :  '1',  # default = 0 / 일간지 = 1 \n",
    "            quote_plus('office_section_code') : '1',  # 언론사 선택하면 1, default = 0\n",
    "            \n",
    "            ########################\n",
    "            ### 신문사 코드 입력 ###\n",
    "            ########################\n",
    "            \n",
    "            quote_plus('news_office_checked') : '1020',  #동아일보 1020,  만일 없으면 공백으로 비워두어야 함\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "    # 첫번째 반복시행에서 article index는 1 고정\n",
    "    first_article_index = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###############################  특정 날짜에서의 검색창 크롤링 ###########################\n",
    "    #iteration_idx = 0\n",
    "    while True:    \n",
    "        try:\n",
    "            #debug code\n",
    "            #iteration_idx +=1\n",
    "            #print('while 작업 횟수 : ', iteration_idx)\n",
    "\n",
    "            '''\n",
    "            5. 뉴스 검색창 접속\n",
    "            '''\n",
    "            # 기타 parameter\n",
    "            start = '&start=' + str(first_article_index)  # 페이지 첫번째 기사의 index, \n",
    "                                    # 처음은 1로 시작하나 페이지 넘길 때마다 10씩 증가시켜야 함\n",
    "                                    # 즉, 날짜별 iteration 코드에 < start += 10 >  를 넣어주어야 함\n",
    "            query_others = '&nso=so%3Ar%2Cp%3Afrom' + str_s_date1 + 'to' + str_e_date1 + '&is_sug_officeid=0' + start\n",
    "\n",
    "            # url\n",
    "            url = base_url + query_params + query_others\n",
    "            # 서버에 접속. 너무 급한 속도의 접속은 IP ban을 유발할 수 있음\n",
    "            req = Request(url, headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "            )\n",
    "            page = urlopen(req)\n",
    "            \n",
    "        except:\n",
    "            t.sleep(20)\n",
    "\n",
    "\n",
    "\n",
    "            '''\n",
    "            5. 뉴스 검색창 접속\n",
    "            '''\n",
    "            # 기타 parameter\n",
    "            start = '&start=' + str(first_article_index)  # 페이지 첫번째 기사의 index, \n",
    "                                    # 처음은 1로 시작하나 페이지 넘길 때마다 10씩 증가시켜야 함\n",
    "                                    # 즉, 날짜별 iteration 코드에 < start += 10 >  를 넣어주어야 함\n",
    "            query_others = '&nso=so%3Ar%2Cp%3Afrom' + str_s_date1 + 'to' + str_e_date1 + '&is_sug_officeid=0' + start\n",
    "\n",
    "            # url\n",
    "            url = base_url + query_params + query_others\n",
    "            # 서버에 접속. 너무 급한 속도의 접속은 IP ban을 유발할 수 있음\n",
    "            req = Request(url, headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "            )\n",
    "            page = urlopen(req)\n",
    "\n",
    "            \n",
    "            \n",
    "        t.sleep(0.3)     # IP ban 방지용\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        6. page parsing\n",
    "        '''\n",
    "        \n",
    "        # bs parsing\n",
    "        page = bs(page, 'html.parser')\n",
    "\n",
    "\n",
    "        # IS_EMPTY 는 검색결과가 있다면 [] 를 반환함. \n",
    "        # 만일 IS_EMPTY 가 []가 아니라면 검색결과가 없는 것이므로 break 해야 함.\n",
    "        IS_EMPTY = page.find_all('div', 'api_noresult_wrap')\n",
    "        if IS_EMPTY != []:\n",
    "            break \n",
    "\n",
    "        # 필요 데이터 parsing (url, 날짜)\n",
    "        url_resultset = page.find_all('div', 'news_info')   #  --> url\n",
    "        news_date_resultset = page.find_all('div', 'info_group')  # --> 날짜,  둘다 parsing 후의 index는 같음\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        7. 지면뉴스 오류 handling\n",
    "        '''\n",
    "        for idx in range(len(url_resultset)):\n",
    "            # url\n",
    "            target_url = url_resultset[idx].a['data-url']\n",
    "\n",
    "            # 날짜\n",
    "            if news_date_resultset[idx].i:\n",
    "                IS_PAPERNEWS = True\n",
    "            else:\n",
    "                IS_PAPERNEWS = False\n",
    "\n",
    "            if IS_PAPERNEWS ==False:\n",
    "                target_date = news_date_resultset[idx].find('span', 'info').text\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    target_date = news_date_resultset[idx].find_all('span', 'info')[1].text\n",
    "                except:\n",
    "                    target_date = news_date_resultset[idx].find_all('span', 'info')[0].text\n",
    "\n",
    "\n",
    "            # 데이터 저장, key = url  / value = date\n",
    "            data_set[target_url] = target_date\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # 다음 검색 페이지로 넘어가기 위해 기사 index 조정\n",
    "        first_article_index += 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 반복문 break에 사용할 current_date 설정\n",
    "    current_date = s_date\n",
    "    current_date += dt.timedelta(days = 1)  # 하루 더하기\n",
    "    current_date = current_date.strftime('%Y%m%d')  # 더한 날짜정보를 str으로 바꾸기\n",
    "\n",
    "    # str으로 변환된 날짜정보를 연,월,일로 분리\n",
    "    year = current_date[:4]\n",
    "    month = current_date[4:6]\n",
    "    day = current_date[6:]\n",
    "\n",
    "    # 다음 iteration에 넘어갈 current_date 정보\n",
    "    current_date = (int(year), int(month), int(day))\n",
    "\n",
    "\n",
    "    # 날짜 while문 종료에 관한 정보\n",
    "    current_date_WHILE = int(dt.datetime(current_date[0], current_date[1], current_date[2]).strftime('%Y%m%d'))\n",
    "\n",
    "'''\n",
    "데이터 내보내기 (url)\n",
    "'''     \n",
    "        \n",
    "table = pd.DataFrame(data_set.keys(), data_set.values())\n",
    "table = table.drop_duplicates()\n",
    "table.to_csv('동아일보 url주소.csv', encoding = 'cp949')    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "########################################\n",
    "##### 2. 본문 크롤링               #####   -> '동아일보_날짜_to_날짜.csv' 생성\n",
    "########################################\n",
    "\n",
    "# url 가져오기 (csv파일)\n",
    "\n",
    "urls = list(pd.read_csv('동아일보 url주소.csv', encoding = 'cp949')['0'])\n",
    "\n",
    "# 데이터가 들어갈 공간\n",
    "data_set = {}\n",
    "\n",
    "\n",
    "\n",
    "#url 접속 후 크롤링\n",
    "\n",
    "\n",
    "for idx in range(len(urls)):\n",
    "    # 차단 막기 위해 잠시 휴식\n",
    "    #t.sleep(0.3)\n",
    "    # 초기화\n",
    "    body = ''\n",
    "    title = ''\n",
    "    day = ''\n",
    "    company = ''\n",
    "\n",
    "    \n",
    "\n",
    "    # 동아일보\n",
    "    if 'donga' in urls[idx]:\n",
    "        \n",
    "        \n",
    "        # 동아일보 계열 타 신문사 제외 (나중에 추가하기)\n",
    "        if 'dongascience' in urls[idx]:\n",
    "            continue\n",
    "        if 'edu.donga' in urls[idx]:\n",
    "            continue\n",
    "        if 'weekly' in urls[idx]:\n",
    "            continue\n",
    "        if 'it.donga' in urls[idx]:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        ### parsing ###\n",
    "        try:\n",
    "            req = Request(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "            )\n",
    "            t.sleep(0.3)\n",
    "            page = urlopen(req)\n",
    "            page = bs(page, 'html.parser')  \n",
    "            \n",
    "        except urllib.error.HTTPError:\n",
    "            continue\n",
    "            \n",
    "        except:\n",
    "            t.sleep(20)\n",
    "            req = Request(urls[idx], headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\n",
    "            )\n",
    "            t.sleep(0.3)\n",
    "            page = urlopen(req)\n",
    "            page = bs(page, 'html.parser')  \n",
    "            \n",
    "            \n",
    "        ### 데이터 저장 ###\n",
    "        body = page.body.find_all('div', 'article_view')[0].find_all('div', 'article_txt')[0].text.split('donga.com')[0].rsplit('기자')[0].lstrip()\n",
    "        title = page.find_all('h1', 'title')[0].text\n",
    "        day = page.find_all('span', 'date01')[0].text.split(' ')[1]\n",
    "        company = '동아일보'\n",
    "        data_set[idx] = [day, title, body, company, urls[idx]]\n",
    "        \n",
    "        \n",
    "    # 다른신문사는 제외 (exception 예방)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "'''\n",
    "데이터 내보내기 (뉴스본문)\n",
    "'''\n",
    "dataframe = pd.DataFrame(data_set).transpose()\n",
    "dataframe.columns = ['date', 'title', 'body', 'company', 'url']\n",
    "# 본문 내 특수문자 제거\n",
    "dataframe['body'] = dataframe['body'].str.replace(pat=r'[^\\w\\s\\.\\%\\)\\(\\']', repl=r'', regex=True)\n",
    "dataframe['body'] = dataframe['body'].str.replace(pat=r'[\"\\xa0\"]', repl = r'', regex=True)\n",
    "# 한자 제거\n",
    "dataframe['body'] = dataframe['body'].str.replace(pat=r'[\\u3400-\\u9FBF]', repl = r'', regex=True)\n",
    "dataframe['body']= dataframe['body'].str.replace(pat=r'[\"\\u2027\",\"\\u2026\"]', repl = r'', regex=True)\n",
    "dataframe['body'] = dataframe['body'].str.replace(' b ', '')\n",
    "\n",
    "\n",
    "\n",
    "dataframe = dataframe.reset_index()\n",
    "dataframe.to_csv('동아일보_{}to{}.csv'.format(first_date, end_date), encoding = \"cp949\", index= False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################\n",
    "end_time = t.time()\n",
    "duration = round((end_time - start_time)/60, 2)\n",
    "\n",
    "\n",
    "print('작업 완료!')\n",
    "print('작업 소요시간 : 약 {}분'.format(duration))\n",
    "print('작업 완료 시간 : ', str(dt.datetime.now()).split(' ')[0], str(dt.datetime.now()).split(' ')[1].split('.')[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 오류 발생 시 어디 문제인지 확인하기 위한 코드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "마지막으로 접속 시도하던 url :  http://www.donga.com/news/article/all/20200210/99625073/1\n"
     ]
    }
   ],
   "source": [
    "print('마지막으로 접속 시도하던 url : ', urls[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
