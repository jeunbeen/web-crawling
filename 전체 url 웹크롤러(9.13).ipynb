{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\r\n",
    "# install pakages  (  -> first time )\r\n",
    "\r\n",
    "'''\r\n",
    "!pip install urllib3\r\n",
    "!pip install bs4\r\n",
    "!pip install pandas\r\n",
    "!pip install tqdm\r\n",
    "\r\n",
    "'''\r\n",
    "\r\n",
    "# 기본 세팅\r\n",
    "\r\n",
    "from urllib import error\r\n",
    "from urllib.request import Request, urlopen\r\n",
    "from urllib.parse import quote, urlencode, quote_plus, unquote\r\n",
    "import json\r\n",
    "from bs4 import BeautifulSoup as bs\r\n",
    "\r\n",
    "\r\n",
    "import copy\r\n",
    "import os\r\n",
    "import datetime as dt\r\n",
    "import time as t\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import random as rd\r\n",
    "\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "'''1. 목표 날짜 지정  (검색할 날짜)'''\r\n",
    "# 검색 시작 날짜\r\n",
    "first_date = 20200101\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "first_date_dt = dt.datetime(int(str(first_date)[:4]) , int(str(first_date)[4:6]), int(str(first_date)[6:]))\r\n",
    "first_date_str = first_date_dt.strftime('%Y%m%d')\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# 검색 종료 날짜\r\n",
    "end_date = 20210120\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "end_date_dt = dt.datetime(int(str(end_date)[:4]) , int(str(end_date)[4:6]), int(str(end_date)[6:]))\r\n",
    "end_date_str = end_date_dt.strftime('%Y%m%d')\r\n",
    "\r\n",
    "# 검색할 날짜 수 계산 (type : datetime)\r\n",
    "number_of_days = (end_date_dt - first_date_dt).days"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "start_time = t.time()\r\n",
    "\r\n",
    "########################################\r\n",
    "##### 1. url 크롤링                #####   -> '한겨레 url주소.csv' 생성\r\n",
    "########################################\r\n",
    "\r\n",
    "data_set = {}          # 데이터가 담길 dictionary\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "print(\"### url 수집 ###\")\r\n",
    "\r\n",
    "'''1. 날짜 하루씩 추가하면서 크롤링 진행'''\r\n",
    "# '날짜 지정'에서 설정한 검색종료 날짜까지 작업 반복\r\n",
    "for day in tqdm(range(number_of_days + 1)) :\r\n",
    "    \r\n",
    "    # current date (datetime / str )\r\n",
    "    current_date_dt = first_date_dt + dt.timedelta(days = day)  # 처음 날짜에서 day만큼 진행\r\n",
    "    current_date_str = current_date_dt.strftime('%Y%m%d')\r\n",
    "    '''\r\n",
    "    2. current_date 를 기반으로 날짜 변수를 datetime 형태로 저장  -\r\n",
    "    ->    이후의 날짜 계산을 용이하게 하기 위함\r\n",
    "    '''\r\n",
    "    s_date = current_date_dt \r\n",
    "    e_date = s_date + dt.timedelta(days=1)   \r\n",
    "\r\n",
    "\r\n",
    "    '''\r\n",
    "    3. datetime 날짜의 type 조작   -->    검색창에 입력할 수 있는 형태로 변환\r\n",
    "    '''\r\n",
    "\r\n",
    "    # 1.  20200201  형태로 저장\r\n",
    "    str_s_date1 = s_date.strftime('%Y%m%d')\r\n",
    "    str_e_date1 = e_date.strftime('%Y%m%d')\r\n",
    "\r\n",
    "    # 2,  2020.02.01 형태로 저장\r\n",
    "    str_s_date2 = s_date.strftime('%Y.%m.%d')\r\n",
    "    str_e_date2 = e_date.strftime('%Y.%m.%d')\r\n",
    "\r\n",
    "\r\n",
    "    '''\r\n",
    "    4. url 및 검색 parameter 지정\r\n",
    "    '''\r\n",
    "\r\n",
    "    # base url\r\n",
    "    base_url = 'https://search.naver.com/search.naver'\r\n",
    "\r\n",
    "    # parameters\r\n",
    "    query_params = '?' + urlencode(\r\n",
    "        {\r\n",
    "            quote_plus('where') : 'news',\r\n",
    "            quote_plus('query') : ' \"웹보드게임\" | \"웹 보드게임\" ',\r\n",
    "            quote_plus('sm') : 'tab_opt',\r\n",
    "            quote_plus('') : '',\r\n",
    "            quote_plus('sort')  : '2',\r\n",
    "            quote_plus('photo') : '0',\r\n",
    "            quote_plus('field') : '0',\r\n",
    "            quote_plus('pd') : '3',\r\n",
    "            quote_plus('ds') : str_s_date2,\r\n",
    "            quote_plus('de') : str_e_date2,\r\n",
    "            quote_plus('docid') : '',  \r\n",
    "            quote_plus('related') : '0',\r\n",
    "            quote_plus('mynews') : '0',       # 언론사 선택시 1 / default = 0 \r\n",
    "            quote_plus('office_type') :  '0',  # default = 0 / 일간지 = 1 \r\n",
    "            quote_plus('office_section_code') : '0',  # 언론사 선택하면 1, default = 0\r\n",
    "            \r\n",
    "            ########################\r\n",
    "            ### 신문사 코드 입력 ###\r\n",
    "            ########################\r\n",
    "            \r\n",
    "            quote_plus('news_office_checked') : '',  # 언론사 선택 안할 땐 공백으로 비워두기\r\n",
    "        }\r\n",
    "    )\r\n",
    "\r\n",
    "\r\n",
    "    # 첫번째 반복시행에서 article index는 1 고정\r\n",
    "    first_article_index = 1\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    ###############################  특정 날짜에서의 검색창 크롤링 ###########################\r\n",
    "    #iteration_idx = 0\r\n",
    "    while True:    \r\n",
    "        try:\r\n",
    "            '''\r\n",
    "            5. 뉴스 검색창 접속\r\n",
    "            '''\r\n",
    "            # 기타 parameter\r\n",
    "            start = '&start=' + str(first_article_index)  # 페이지 첫번째 기사의 index, \r\n",
    "                                    # 처음은 1로 시작하나 페이지 넘길 때마다 10씩 증가시켜야 함\r\n",
    "                                    # 즉, 날짜별 iteration 코드에 < start += 10 >  를 넣어주어야 함\r\n",
    "            query_others = '&nso=so%3Ar%2Cp%3Afrom' + str_s_date1 + 'to' + str_e_date1 + '&is_sug_officeid=0' + start\r\n",
    "\r\n",
    "            # url\r\n",
    "            url = base_url + query_params + query_others\r\n",
    "            # 서버에 접속. 너무 급한 속도의 접속은 IP ban을 유발할 수 있음\r\n",
    "            req = Request(url, headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\r\n",
    "            )\r\n",
    "            page = urlopen(req)\r\n",
    "            \r\n",
    "        except:\r\n",
    "            t.sleep(20)\r\n",
    "\r\n",
    "            '''\r\n",
    "            5. 뉴스 검색창 접속\r\n",
    "            '''\r\n",
    "            # 기타 parameter\r\n",
    "            start = '&start=' + str(first_article_index)  # 페이지 첫번째 기사의 index, \r\n",
    "                                    # 처음은 1로 시작하나 페이지 넘길 때마다 10씩 증가시켜야 함\r\n",
    "                                    # 즉, 날짜별 iteration 코드에 < start += 10 >  를 넣어주어야 함\r\n",
    "            query_others = '&nso=so%3Ar%2Cp%3Afrom' + str_s_date1 + 'to' + str_e_date1 + '&is_sug_officeid=0' + start\r\n",
    "\r\n",
    "            # url\r\n",
    "            url = base_url + query_params + query_others\r\n",
    "            # 서버에 접속. 너무 급한 속도의 접속은 IP ban을 유발할 수 있음\r\n",
    "            req = Request(url, headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'}\r\n",
    "            )\r\n",
    "            page = urlopen(req)\r\n",
    "\r\n",
    "            \r\n",
    "            \r\n",
    "        t.sleep(0.3)     # IP ban 방지용\r\n",
    "        \r\n",
    "\r\n",
    "        \r\n",
    "        \r\n",
    "        '''\r\n",
    "        6. page parsing\r\n",
    "        '''\r\n",
    "        \r\n",
    "        # bs parsing\r\n",
    "        page = bs(page, 'html.parser')\r\n",
    "\r\n",
    "\r\n",
    "        # IS_EMPTY 는 검색결과가 있다면 [] 를 반환함. \r\n",
    "        # 만일 IS_EMPTY 가 []가 아니라면 검색결과가 없는 것이므로 break 해야 함.\r\n",
    "        IS_EMPTY = page.find_all('div', 'api_noresult_wrap')\r\n",
    "        if IS_EMPTY != []:\r\n",
    "            break \r\n",
    "\r\n",
    "        # 필요 데이터 parsing (url, 날짜)\r\n",
    "        url_resultset = page.find_all('div', 'news_info')   #  --> url\r\n",
    "        news_date_resultset = page.find_all('div', 'info_group')  # --> 날짜,  둘다 parsing 후의 index는 같음\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "        '''\r\n",
    "        7. 지면뉴스 오류 handling\r\n",
    "        '''\r\n",
    "        for idx in range(len(url_resultset)):\r\n",
    "            # url\r\n",
    "            target_url = url_resultset[idx].a['data-url']\r\n",
    "\r\n",
    "            # 날짜\r\n",
    "            if news_date_resultset[idx].i:\r\n",
    "                IS_PAPERNEWS = True\r\n",
    "            else:\r\n",
    "                IS_PAPERNEWS = False\r\n",
    "\r\n",
    "            if IS_PAPERNEWS ==False:\r\n",
    "                target_date = news_date_resultset[idx].find('span', 'info').text\r\n",
    "\r\n",
    "            else:\r\n",
    "                try:\r\n",
    "                    target_date = news_date_resultset[idx].find_all('span', 'info')[1].text\r\n",
    "                except:\r\n",
    "                    target_date = news_date_resultset[idx].find_all('span', 'info')[0].text\r\n",
    "\r\n",
    "\r\n",
    "            # 데이터 저장, key = url  / value = date\r\n",
    "            data_set[target_url] = target_date\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "        # 다음 검색 페이지로 넘어가기 위해 기사 index 조정\r\n",
    "        first_article_index += 10\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "'''\r\n",
    "데이터 내보내기 (url)\r\n",
    "'''     \r\n",
    "        \r\n",
    "table = pd.DataFrame(data_set.keys(), data_set.values())\r\n",
    "table = table.drop_duplicates()\r\n",
    "table.to_csv('전체기사 url주소.csv' , encoding = 'cp949')    \r\n",
    "    \r\n",
    "\r\n",
    "\r\n",
    "    \r\n",
    "    \r\n",
    "    \r\n",
    "\r\n",
    "\r\n",
    "################################################\r\n",
    "end_time = t.time()\r\n",
    "duration = round((end_time - start_time)/60, 2)\r\n",
    "\r\n",
    "\r\n",
    "print('작업 완료!')\r\n",
    "print('작업 소요시간 : 약 {}분'.format(duration))\r\n",
    "print('작업 완료 시간 : ', str(dt.datetime.now()).split(' ')[0], str(dt.datetime.now()).split(' ')[1].split('.')[0])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "### url 수집 ###\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 386/386 [07:15<00:00,  1.13s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "작업 완료!\n",
      "작업 소요시간 : 약 7.26분\n",
      "작업 완료 시간 :  2021-09-14 10:54:55\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "raw",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('nlp': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "interpreter": {
   "hash": "3384638f5810346ec4e47c085d16c91a672dec4d6598dd6288d516a5e6f3bebf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}